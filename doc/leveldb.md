# [leveldb](https://leveldb-handbook.readthedocs.io/zh/latest/basic.html)
  
### 关系型数据库：
　　关系型数据库，是指采用了关系模型来组织数据的数据库，最大特点就是事务的一致性。简单来说，关系模型指的就是二维表格模型，而一个关系型数据库就是由二维表及其之间的联系所组成的一个数据组织。
#### 优点
  * 容易理解：二维表结构是非常贴近逻辑世界一个概念，关系模型相对网状、层次等其他模型来说更容易理解；
  * 使用方便：通用的SQL语言使得操作关系型数据库非常方便；
  * 易于维护：丰富的完整性(实体完整性、参照完整性和用户定义的完整性)大大减低了数据冗余和数据不一致的概率；
#### 瓶颈
  * 高并发读写需求：网站的用户并发性非常高，往往达到每秒上万次读写请求，对于传统关系型数据库来说，硬盘I/O是一个很大的瓶颈
  * 海量数据的高效率读写：网站每天产生的数据量是巨大的，对于关系型数据库来说，在一张包含海量数据的表中查询，效率是非常低的
  * 高扩展性和可用性：在基于web的结构当中，数据库是最难进行横向扩展的，当一个应用系统的用户量和访问量与日俱增的时候，数据库却没有办法像web server和app server那样简单的通过添加更多的硬件和服务节点来扩展性能和负载能力。对于很多需要提供24小时不间断服务的网站来说，对数据库系统进行升级和扩展 是非常痛苦的事情，往往需要停机维护和数据迁移。
  
### 对网站来说，关系型数据库的很多特性不再需要
  * 事务一致性：关系型数据库在对事物一致性的维护中有很大的开销，而现在很多web2.0系统对事物的读写一致性都不高。
  * 读写实时性：对关系数据库来说，插入一条数据之后立刻查询，是肯定可以读出这条数据的，但是对于很多web应用来说，并不要求这么高的实时性，比如发一条消息之后，过几秒乃至十几秒之后才看到这条动态是完全可以接受的。
  * 复杂SQL，特别是多表关联查询：任何大数据量的web系统，都非常忌讳多个大表的关联查询，以及复杂的数据分析类型的复杂SQL报表查询，特别是SNS类型的网站，从需求以及产品阶级角度，就避免了这种情况的产生。往往更多的只是单表的主键查询，以及单表的简单条件分页查询，SQL的功能极大的弱化了。
  * 在关系型数据库中，导致性能欠佳的最主要原因是多表的关联查询，以及复杂的数据分析类型的复杂SQL报表查询。为了保证数据库的ACID特性，我们必须尽量按照其要求的范式进行设计，关系型数据库中的表都是存储一个格式化的数据结构。每个元组字段的组成都是一样，即使不是每个元组都需要所有的字段， 但数据库会为每个元组分配所有的字段，这样的结构可以便于标语表之间进行链接等操作，但从另一个角度来说它也是关系型数据库性能瓶颈的一个因素。
  
### 非关系型数据库出现的原因
  * 关系型数据库的最大特点就是事务的一致性：传统的关系型数据库读写操作都是事务的，具有ACID的特点，这个特性使得关系型数据库可以用于几乎所有对一致性有要求的系统中，如典型的银行系统。但是，在网页应用中，尤其是SNS应用中，一致性却不是显得那么重要，比如，两个人看到同一好友的数据更新的时间差那么几秒是可以容忍的，因此，关系型数据库的最大特点在这里已经无用武之地，起码不是那么重要了。
  * 相反地，关系型数据库为了维护一致性所付出的巨大代价就是其读写性能比较差，而像微博、facebook这类SNS的应用，对并发读写能力要求极高，关系型数据库已经无法应付（在读方面，传统上为了克服关系型数据库缺陷，提高性能，都是增加一级memcache来静态化网页，而在SNS中，变化太快，memchache已经无能为力了），因此，必须用新的一种数据结构存储来代替关系数据库。
  * 关系数据库的另一个特点就是其具有固定的表结构，因此，其扩展性极差，而在SNS中，系统的升级，功能的增加，往往意味着数据结构巨大变动，这一点关系型数据库也难以应付，需要新的结构化数据存储。
  * 于是，非关系型数据库应运而生，由于不可能用一种数据结构化存储应付所有的新的需求，因此，非关系型数据库严格上不是一种数据库，应该是一种数据结构化存储方法的集合。必须强调的是，数据的持久存储，尤其是海量数据的持久存储，还是需要关系数据库。
  
### 非关系型数据库
　　NoSQL一词，用于指代那些非关系型的，分布式的，且一般不保证遵循 ACID 原则的数据存储系统（NoSQL遵循的是BASE原则）。非关系型数据库严格上不是一种数据库，应该是一种数据结构化存储方法的集合，是传统关系型数据库的功能阉割版本。直接使用键值对存储数据，一般不支持ACID特性多用于分布式场景中。
#### 优点
  * 并发读写性能强悍：通过减少用不到或很少用的功能（如无需经过sql层的解析），来大幅度提高读写性能，适合高并发的海量数据的高效率读写需求。
  * 扩展性好：没有固定的表结构，基于键值对结构存储，数据没有耦合性，容易水平扩展；
  * 存储数据的格式多样：nosql的存储格式是key-value形式、文档形式、图片形式等等，文档形式、图片形式等等，关系型数据库则只支持基础类型。
#### 瓶颈
  * 非关系型数据库由于很少的约束，只适合存储一些较为简单的数据，不能够提供像 SQL 所提供的 where 这种对于字段属性值情况的查询。对于需要进行较复杂查询的数据，SQL 数据库显的更为合适。

### NoSQL遵循BASE原则
  * 基本可用（Basically Availble）：在绝大多数时间内可用，支持分区失败（Sharding碎片划分数据库），出了问题服务仅降级（部分不可用）。
  * 软状态/柔性（Soft-state）：事务"Soft state" 可以理解为"无连接"的, 而 "Hard state" 是"面向连接"的。 软状态就是数据状态不要求任意时刻都保持同步，可以有一段时间不同步。
  * 最终一致性（Eventual Consistency）：最终数据是一致的就可以了。在一定的时间内数据会达到一致的状态，而不是时时一致。

### 日志结构合并树（Log-Structured Merge Tree）
  * LSM Tree是为了优化数据库写性能而出现的。在牺牲了数据库一定的读能力的条件下，极大改善了数据库的写能力。它要求一棵树始终位于内存（小树），另一棵树位于磁盘（大树），小树可以是红黑树、跳跃表（参考LevelDB），甚至可以是B树，而大树则通常是B树或其变种，所以LSM Tree中的索引是可以选择的。
  * **传统的关系数据库使用B+树最大的性能问题是会产生大量的随机IO，随着新数据的插入，叶子节点会慢慢分裂，逻辑上连续的叶子节点在物理上往往不连续，甚至分离的很远，做范围查询时，会产生大量读随机写**，因为你无法保证节点常驻内存，尤其是当B+树管理的索引量很大的时候。这导致数据库读写性能急剧下降。
  * LSM树的设计思想是把一颗大树拆分成N棵小树，它首先写入到内存中（内存没有寻道速度的问题，随机写的性能得到大幅提升），在内存中构建一颗有序小树，随着小树越来越大，内存的小树会flush到磁盘上。**磁盘中的树定期可以做merge操作，合并成一棵大树以优化读性能。无论是内存的小树flush到磁盘，还是磁盘中的树合并，都是顺序写为主，极大程度地避免了磁盘随机写，因此具有写入速度快的特点**。
  
### LSM Tree与level的关系
  * LSM存储模型：牺牲读性能，提高随机写性能；
  * Level存储架构：尽可能地优化读性能，同时减少对写性能的影响。假设没有Level的概念，每次读请求都要去访问多个文件，于是才有Level的概念去做compaction，尽可能减少读取的文件数，同时又保证了每次Compaction IO的数据量，保证对正常的写请求影响不会太大；
  * LSM和Level之间是相互均衡的关系，它们决定了读写性能。在不同的应用场景下，我们需要在两者间有所取舍。

### LSM Tree读性能如何保证
　　LSM Tree放弃磁盘读性能来换取写的顺序性，但不代表LSM Tree的读性能就不理想。
  * 内存的速度远超磁盘，1000倍以上。而读取的性能提升，主要还是依靠内存命中率而非磁盘读的次数。
  * 写入占用更少磁盘的IO，读取就能获取更长时间的磁盘IO使用权，从而也可以提升读取效率。例如LevelDb的SSTable虽然降低了了读的性能，但如果数据的读取命中率有保障的前提下，因为读取能够获得更多的磁盘IO机会，因此读取性能基本没有降低，甚至还会有提升。而写入的性能则会获得较大幅度的提升，基本上是5~10倍左右。
  
### 跳跃表（Skiplist）
 * 跳跃表最大的优点在于实现比RB-Tree、AVL等简单很多。
 * 虽然跳跃表的时间复杂度和RB-Tree可以说相差不大，都是O(logN)，但插入速度非常快速，因为不需要进行旋转等操作来维护平衡性。
 * 在高并发场景下，跳跃表通过内存屏障实现无锁的多读一写；如果涉及多写，需要加锁实现同步，但跳跃表没有繁琐的平衡过程，更新的部分相对较少，多线程竞争锁的代价就会减少，因此在高并发的环境下跳跃表的性能比RB-Tree要好。
 * 跳跃表每次都是靠随机函数产生随机层数，所以不太稳定。
 * 跳跃表需要存储的节点更多，内存消耗更大。

### leveldb的Put写过程
  * `DB::Put()`操作会将（key,value）转化成writebatch后，通过`DBImpl::Write()`接口来完成
  * 在`DBImpl::Write()`之前需要通过`DBImpl::MakeRoomForWrite()`来保证MemTable有空间来接受写请求，这个过程中可能阻塞写请求，以及进行Compaction;
  * `DBImpl::BuildBatchGroup()`类似于缓冲区，尽可能的将多个writebatch合并在一起然后写下去，能够提升吞吐量;
  * `Log::Write:AddRecord()`就是在写入MemTable之前，先在操作写入到Log文件中;
  * 最后`WriteBatchInternal::InsertInto()`会将数据写入到MemTable中。

### leveldb的Get读过程
  * 首先判断options.snapshot是否为空，如果为非空，snapshot值就取这个值，否则取最新数据的版本号;
  * 依次尝试去内存中的MemTable和Immutable MemTable中查找；
  * 在VersionSet中去查找：先逐层查找，确定key可能所在的文件。**L0从文件编号大的sstable优先查找，因为文件编号较大的sstable中存储的总是最新的数据。非0层文件，一层中所有文件之间的key不重合，因此leveldb可以借助sstable的元数据（一个文件中最小与最大的key值）进行快速定位，每一层只需要查找一个sstable文件的内容**。
  * 然后根据文件编号，在TableCache中查找，如果未命中，会将Table信息Load到cache中。再根据Table信息，确定key可能所在的Block。最后在BlockCache中查找Block，如果未命中，会将Block load到Cache中。然后在Block内查找key是否命中。
  * 更新读数据的统计信息，作为一个ssTable是否应该进行Compaction的依据。
  * 最后释放对Memtable，Immutable MemTable，VersionSet的引用

### leveldb对LSM Tree读性能的优化
  * Bloom filter：就是个带随即概率的bitmap,可以快速的得知某一个小的有序结构里有没有指定的数据。如果没有就不必进行二分查找，而只需简单的计算几次就能知道数据所在的ssTable，通过空间换取时间，效率得到了提升。
  * Compact：将小树合并为大树：在大树上使用二分查找的效率比在多颗小树下进行二分查找要快得多（m棵小树需要`(N/m)*logN`）。
 
### leveldb的Compaction合并过程
　　在leveldb中compaction主要包括Manual Compaction和Auto Compaction，在Auto Compaction中又包含了MemTable Compaction和SSTable Compaction。
### Manual Compaction
　　leveldb中manual compaction是用户指定需要做compaction的key range，调用接口CompactRange来实现，它的主要流程为：
  * 计算和Range有重合的MaxLevel；
  * 从level 0 到 MaxLevel依次在每层对这个Range做Compaction；
  * 做Compaction时会限制选择做Compaction文件的大小，这样可能每个level的CompactRange可能需要做多次Compaction才能完成。
### SSTable Compaction
##### 启动条件（条件1的优先级高于条件2）
  * 每个Level的文件大小或文件数超过了这个Level的限制（L0对比文件个数，其它Level对比文件大小。主要是因为L0文件之间可能重叠，文件过多影响读访问，而其它level文件不重叠，限制文件总大小，可以防止一次compaction IO过重）；
  * 含有被寻道次数超过一定阈值的文件(这个是指读请求查找可能去读多个文件，如果最开始读的那个文件未查找到，那么这个文件就被认为寻道一次，当文件的寻道次数达到一定数量时，就认为这个文件应该去做compaction)。
##### 触发条件
  * 任何改变了上面两个条件的操作，都会触发Compaction，即调用MaybeScheduleCompaction；
  * 涉及到第一个条件改变，就是会改变某层文件的文件数目或大小，而只有Compaction操作之后才会改变这个条件；
  * 涉及到第二个条件的改变，可能是读操作和scan操作(scan操作是每1M数据采样一次，获得读最后一个key所寻道的文件，1M数据的cost大约为一次寻道)。
##### 文件选取
  * 每个level都会记录上一次Compaction选取的文件所含Key的最大值，作为下次compaction选取文件的起点；
  * 对于根据启动条件1所做的Compaction，选取文件就从上次的点开始选取，这样保证每层每个文件都会选取到；
  * 对于根据启动条件2所做的Compaction，需要做compaction的文件本身就已经确定了Level + 1层文件的选取，就是和level层选取的文件有重合的文件；
  * 在leveldb中在L层会选取1个文件，理论上这个文件最多覆盖的文件数为12个（leveldb中默认一个文件最大为2M，每层的最大数据量按照10倍增长。这样L层的文件在未对齐的情况下最多覆盖L+1层的12个文件），这样可以控制一次Compaction的最大IO为（1+12）* 2M读IO，总的IO不会超过52M。
### MemTable Compaction
　　MemTable Compaction最重要的是产出的文件所在层次的选择，它必须满足如下条件： 假设最终选择层次L，那么文件必须和`[0, L-1]`所有层的文件都没有重合，且对L+1层文件的覆盖不能超过一定的阈值（保证Compaction IO可控)。
### Compaction文件产出时机
  * 文件大小达到一定的阈值；
  * 产出文件对Level+2层有交集的所有文件的大小超过一定阈值。
### Compaction时key丢弃的两个条件
  * last_sequence_for_key <= smallest_snapshot (有一个更新的同样的user_key比最小快照要小）；
  * key_type == del && key <= smallest_snapshot && IsBaseLevelForKey（key的类型是删除，且这个key的版本比最小快照要小，并且在更高Level没有同样的user_key)。

### Rocksdb对leveldb的优化
  * [RocksDB相比LevelDB的新特性](http://xiaqunfeng.cc/2017/02/23/RocksDB%E7%9B%B8%E6%AF%94LevelDB%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7/)
  * RocksDB支持一次获取多个K-V，还支持Key范围查找。LevelDB只能获取单个Key；
  * RocksDB提供一些方便的工具，这些工具包含解析sst文件中的K-V记录、解析MANIFEST文件的内容等。有了这些工具，就不用再像使用LevelDB那样，只能在程序中才能知道sst文件K-V的具体信息了。
  * RocksDB支持多线程合并，而LevelDB是单线程合并的。LSM型的数据结构，最大的性能问题就出现在其合并的时间损耗上，在多CPU的环境下，多线程合并那是LevelDB所无法比拟的。不过据其官网上的介绍，似乎多线程合并还只是针对那些与下一层没有Key重叠的文件，只是简单的rename而已，至于在真正数据上的合并方面是否也有用到多线程，就只能看代码了。
  * RocksDB增加了合并时过滤器，对一些不再符合条件的K-V进行丢弃，如根据K-V的有效期进行过滤。
  * 压缩方面RocksDB可采用多种压缩算法，除了LevelDB用的snappy，还有zlib、bzip2。LevelDB里面按数据的压缩率（压缩后低于75%）判断是否对数据进行压缩存储，而RocksDB典型的做法是Level 0-2不压缩，最后一层使用zlib，而其它各层采用snappy。
  * 在故障方面，RocksDB支持增量备份和全量备份，允许将已删除的数据备份到指定的目录，供后续恢复。
  * RocksDB支持在单个进程中启用多个实例，而LevelDB只允许单个实例。
  * RocksDB支持管道式的Memtable，也就说允许根据需要开辟多个Memtable，以解决Put与Compact速度差异的性能瓶颈问题。在LevelDB里面因为只有一个Memtable，如果Memtable满了却还来不及持久化，这个时候LevelDB将会减缓Put操作，导致整体性能下降。



